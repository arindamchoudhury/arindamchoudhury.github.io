## Batch, Mini-batch, and Stochastic Gradient Descent
- If we use all points in the training set (n = N) to compute the loss, we are performing a batch gradient descent;
- If we were to use a single point (n = 1) each time, it would be a stochastic gradient descent;
- Anything else (n) in between 1 and N characterizes a minibatch gradient descent;

## Scaling / Standardizing / Normalizing
transforms a feature in such a way that it ends up with zero mean and unit standard deviation.

First, it computes the mean and the standard deviation of a given feature (x) using the training set (N points):

$$ \overline X = \frac{1}{N}\sum_{i=1}^{N}x_i $$
$$ \sigma(X) = \sqrt {\frac{1}{N}\sum_{i=1}^{N}({x_i - \overline X})^2} $$

Then, it uses both values to scale the feature:

$$  \text{scaled } x_i =  \frac{x_i - \overline X }{\sigma(X)} $$

If we were to recompute the mean and the standard deviation of the scaled feature, we would get 0 and 1, respectively. This pre-processing step is commonly referred to as normalization, although, technically, it should always be referred to as standardization.

> IMPORTANT: 
> Pre-processing steps like the StandardScaler MUST be performed AFTER the train-validation-test split; otherwise, you’ll be leaking information from the 
> validation and/or test sets to your model!
> After using the training set only to fit the StandardScaler, you should use its transform() method to apply the pre-processing step to all datasets: 
> training, validation, and test.

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler(with_mean=True, with_std=True)
# We use the TRAIN set ONLY to fit the scaler
scaler.fit(x_train)
scaled_x_train = scaler.transform(x_train)
scaled_x_val = scaler.transform(x_val)
```
Notice that we are not regenerating the data—we are using the original feature x as input for the StandardScaler and transforming it into a scaled x. The labels (y) are left untouched.

## Definition of Epoch
An epoch is complete whenever every point in the training set (N) has already been used in all steps: forward pass, computing loss, computing gradients, and updating parameters. During one epoch, we perform at least one update, but no more
than N updates.
The number of updates (N/n) will depend on the type of gradient descent being used:
- For batch (n = N) gradient descent, this is trivial, as it uses all points for computing the loss—one epoch is the same as one update.
- For stochastic (n = 1) gradient descent, one epoch means N updates, since every individual data point is used to perform an update.
- For mini-batch (of size n), one epoch has N/n updates, since a mini-batch of n data points is used to perform an update.