```python
import numpy as np
from sklearn.linear_model import LinearRegression

import torch
import torch.optim as optim
import torch.nn as nn
from torchviz import make_dot
```

![!\[alt text\](image.png)](images/image.png)

## Data Generation
```python
true_b = 1
true_w = 2
N = 100

# Data Generation
np.random.seed(42)
x = np.random.rand(N, 1)
epsilon = (.1 * np.random.randn(N, 1))
y = true_b + true_w * x + epsilon

idx = np.arange(N)
np.random.shuffle(idx)

# Uses first 80 random indices for train
train_idx = idx[:int(N*.8)]
# Uses the remaining indices for validation
val_idx = idx[int(N*.8):]

# Generates train and validation sets
x_train, y_train = x[train_idx], y[train_idx]
x_val, y_val = x[val_idx], y[val_idx]
```

## Linear Regression
For each epoch, there are four training steps
- Compute model’s predictions—this is the forward pass
- Compute the loss, using predictions and labels and the appropriate loss function for the task at hand
- Compute the gradients for every parameter
- Update the parameters

```python
np.random.seed(42)
b = np.random.randn(1)
w = np.random.randn(1)

print(b, w)

# Sets learning rate - this is "eta" ~ the "n"-like Greek letter
lr = 0.1
# Defines number of epochs
n_epochs = 1000

for epoch in range(n_epochs):
    # Step 1 - Computes model's predicted output - forward pass
    yhat = b + w * x_train
    
    # Step 2 - Computes the loss
    # We are using ALL data points, so this is BATCH gradient
    # descent. How wrong is our model? That's the error!   
    error = (yhat - y_train)
    # It is a regression, so it computes mean squared error (MSE)
    loss = (error ** 2).mean()
    
    # Step 3 - Computes gradients for both "b" and "w" parameters
    b_grad = 2 * error.mean()
    w_grad = 2 * (x_train * error).mean()
    
    # Step 4 - Updates parameters using gradients and 
    # the learning rate
    b = b - lr * b_grad
    w = w - lr * w_grad
    
print(b, w)

# Sanity Check: do we get the same results as our
# gradient descent?
linr = LinearRegression()
linr.fit(x_train, y_train)
print(linr.intercept_, linr.coef_[0])
```

## tensors
You can create tensors in PyTorch pretty much the same way you create arrays in Numpy. Using tensor() you can create either a scalar or a tensor.
PyTorch’s tensors have equivalent functions to its Numpy counterparts, like ones(), zeros(), rand(), randn(), and many more.

```python
scalar = torch.tensor(3.14159)
vector = torch.tensor([1, 2, 3])
matrix = torch.ones((2, 3), dtype=torch.float)
tensor = torch.randn((2, 3, 4), dtype=torch.float)

print(scalar)
print(vector)
print(matrix)
print(tensor)
```
You can get the shape of a tensor using its size() method or its shape attribute.
```python
print(tensor.size(), tensor.shape)
print(scalar.size(), scalar.shape)
```
All tensors have shapes, but scalars have "empty" shapes, since they are dimensionless (or zero dimensions, if you prefer).

You can also reshape a tensor using its view() (preferred) or reshape() methods.

> Beware: The view() method only returns a tensor with the desired shape that shares the underlying data with 
> the original tensor—it DOES NOT create a new, independent, tensor!
> The reshape() method may or may not create a copy! The reasons behind this apparently weird behavior are 
> beyond thescope of this section, but this behavior is the reason why view() is preferred.
```python
# We get a tensor with a different shape but it still is
# the SAME tensor
same_matrix = matrix.view(1, 6)
# If we change one of its elements...
same_matrix[0, 1] = 2.
# It changes both variables: matrix and same_matrix
print(matrix)
print(same_matrix)
```

If you want to copy all data, that is, duplicate the data in memory, you may use either its new_tensor() or clone() methods.
It seems that PyTorch prefers that we use clone()—together with detach()—instead of new_tensor(). Both ways accomplish exactly the same result, but the code below is deemed cleaner and more readable.
```python
another_matrix = matrix.view(1, 6).clone().detach()
```

## Loading Data
It is time to start converting our Numpy code to PyTorch: That’s what as_tensor() is good for (which works like from_numpy()).
```python
x_train_tensor = torch.as_tensor(x_train)
x_train.dtype, x_train_tensor.dtype
```
You can also easily cast it to a different type, like a lower-precision (32-bit) float, which will occupy less space in memory, using float():
```python
float_tensor = x_train_tensor.float()
float_tensor.dtype
```

> IMPORTANT: Both as_tensor() and from_numpy() return a tensor that shares the underlying data with the 
> original Numpy array. Similar to what happened when we used view() in the last section, if you modify the 
> original Numpy array, you’re modifying the corresponding PyTorch tensor too, and vice-versa.

just keep in mind that torch.tensor() always makes a copy of the data, instead of sharing the underlying data with the Numpy array.

You can also perform the opposite operation, namely, transforming a PyTorch tensor back to a Numpy array. That’s what numpy() is good for:

```python
dummy_tensor.numpy()
```

## Defining your device

```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'
```
```python
n_cudas = torch.cuda.device_count()
for i in range(n_cudas):
    print(torch.cuda.get_device_name(i))
```
```python
gpu_tensor = torch.as_tensor(x_train).to(device)
gpu_tensor[0]
```

```python
x_train_tensor = torch.as_tensor(x_train).float().to(device)
y_train_tensor = torch.as_tensor(y_train).float().to(device)
print(type(x_train), type(x_train_tensor), x_train_tensor.type())
```

```python
back_to_numpy = x_train_tensor.cpu().numpy()
```

## Creating Parameters
What distinguishes a tensor used for training data (or validation, or test)—like the ones we’ve just created—from a tensor used as a (trainable) parameter / weight?

The latter requires the computation of its gradients, so we can update their values (the parameters’ values, that is). That’s what the requires_grad=True argument isgood for. It tells PyTorch to compute gradients for us.
> A tensor for a learnable parameter requires a gradient!

> In PyTorch, every method that ends with an underscore (_), like the requires_grad_() method above, makes 
> changes in-place, meaning, they will modify the underlying variable.

```python
# We can specify the device at the moment of creation 
# RECOMMENDED!

# Step 0 - Initializes parameters "b" and "w" randomly
torch.manual_seed(42)
b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)
w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)
print(b, w)
```
> Always assign tensors to a device at the moment of their creation to avoid unexpected behaviors!

## Autograd
Autograd is PyTorch’s automatic differentiation package. Thanks to it, we don’t need to worry about partial derivatives, chain rule, or anything like it.

### backward
So, how do we tell PyTorch to do its thing and compute all gradients? That’s the role of the backward() method. It will compute gradients for all (gradient-requiring) tensors involved in the computation of a given variable.
Do you remember the starting point for computing the gradients? It was the loss, as we computed its partial derivatives w.r.t. our parameters. Hence, we need to invoke the backward() method from the corresponding Python variable:
```python
# Step 1 - Computes our model's predicted output - forward pass
yhat = b + w * x_train_tensor

# Step 2 - Computes the loss
# We are using ALL data points, so this is BATCH gradient descent
# How wrong is our model? That's the error! 
error = (yhat - y_train_tensor)
# It is a regression, so it computes mean squared error (MSE)
loss = (error ** 2).mean()

# Step 3 - Computes gradients for both "b" and "w" parameters
# No more manual computation of gradients! 
# b_grad = 2 * error.mean()
# w_grad = 2 * (x_tensor * error).mean()
loss.backward()
```
```python
print(error.requires_grad, yhat.requires_grad, b.requires_grad, w.requires_grad)
print(y_train_tensor.requires_grad, x_train_tensor.requires_grad)
```
```python
print(b.grad, w.grad)
```

We need to use the gradients corresponding to the current loss to perform the parameter update. We should NOT use accumulated gradients. "If accumulating gradients is a problem, why does PyTorch do it by
default?" It turns out this behavior can be useful to circumvent hardware limitations.

During the training of large models, the necessary number of data points in a minibatch may be too large to fit in memory (of the graphics card). How can one solve this, other than buying more-expensive hardware?
One can split a mini-batch into "sub-mini-batches" (horrible name, I know, don’t quote me on this!), compute the gradients for those "subs" and accumulate them to achieve the same result as computing the gradients on the full mini-batch.

### zero_
Every time we use the gradients to update the parameters, we need to zero the gradients afterward. And that’s what zero_() is good for.
```python
b.grad.zero_(), w.grad.zero_()
```

## Updating Parameters
```python
# Sets learning rate - this is "eta" ~ the "n"-like Greek letter
lr = 0.1

# Step 0 - Initializes parameters "b" and "w" randomly
torch.manual_seed(42)
b = torch.randn(1, requires_grad=True, \
                dtype=torch.float, device=device)
w = torch.randn(1, requires_grad=True, \
                dtype=torch.float, device=device)

# Defines number of epochs
n_epochs = 1000

for epoch in range(n_epochs):
    # Step 1 - Computes model's predicted output - forward pass
    yhat = b + w * x_train_tensor
    
    # Step 2 - Computes the loss
    # We are using ALL data points, so this is BATCH gradient
    # descent. How wrong is our model? That's the error!
    error = (yhat - y_train_tensor)
    # It is a regression, so it computes mean squared error (MSE)
    loss = (error ** 2).mean()

    # Step 3 - Computes gradients for both "b" and "w" parameters
    # No more manual computation of gradients! 
    # b_grad = 2 * error.mean()
    # w_grad = 2 * (x_tensor * error).mean()   
    # We just tell PyTorch to work its way BACKWARDS 
    # from the specified loss!
    loss.backward()
    
    # Step 4 - Updates parameters using gradients and 
    # the learning rate. But not so fast...
    # FIRST ATTEMPT - just using the same code as before
    # AttributeError: 'NoneType' object has no attribute 'zero_'
    # b = b - lr * b.grad
    # w = w - lr * w.grad
    # print(b)

    # SECOND ATTEMPT - using in-place Python assigment
    # RuntimeError: a leaf Variable that requires grad
    # has been used in an in-place operation.
    # b -= lr * b.grad
    # w -= lr * w.grad        
    
    # THIRD ATTEMPT - NO_GRAD for the win!
    # We need to use NO_GRAD to keep the update out of
    # the gradient computation. Why is that? It boils 
    # down to the DYNAMIC GRAPH that PyTorch uses...
    with torch.no_grad():
        b -= lr * b.grad
        w -= lr * w.grad
    
    # PyTorch is "clingy" to its computed gradients, we
    # need to tell it to let it go...
    b.grad.zero_()
    w.grad.zero_()
    
print(b, w)
```

## Optimizer
So far, we’ve been manually updating the parameters using the computed gradients. That’s probably fine for two parameters, but what if we had a whole lot of them? We need to use one of PyTorch’s optimizers, like SGD, RMSprop, or Adam.

Remember, the choice of mini-batch size influences the path of gradient descent, and so does the choice of an optimizer.

### step / zero_grad
An optimizer takes the parameters we want to update, the learning rate we want to use (and possibly many other hyper-parameters as well!), and performs the updates through its step() method.
```python
optimizer = optim.SGD([b, w], lr=lr)
```
Besides, we also don’t need to zero the gradients one by one anymore. We just invoke the optimizer’s zero_grad() method, and that’s it!
```python
# Sets learning rate - this is "eta" ~ the "n"-like Greek letter
lr = 0.1

# Step 0 - Initializes parameters "b" and "w" randomly
torch.manual_seed(42)
b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)
w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)

# Defines a SGD optimizer to update the parameters
optimizer = optim.SGD([b, w], lr=lr)

# Defines number of epochs
n_epochs = 1000

for epoch in range(n_epochs):
    # Step 1 - Computes model's predicted output - forward pass
    yhat = b + w * x_train_tensor
    
    # Step 2 - Computes the loss
    # We are using ALL data points, so this is BATCH gradient 
    # descent. How wrong is our model? That's the error! 
    error = (yhat - y_train_tensor)
    # It is a regression, so it computes mean squared error (MSE)
    loss = (error ** 2).mean()

    # Step 3 - Computes gradients for both "b" and "w" parameters
    loss.backward()
    
    # Step 4 - Updates parameters using gradients and 
    # the learning rate. No more manual update!
    # with torch.no_grad():
    #     b -= lr * b.grad
    #     w -= lr * w.grad
    optimizer.step()
    
    # No more telling Pytorch to let gradients go!
    # b.grad.zero_()
    # w.grad.zero_()
    optimizer.zero_grad()
    
print(b, w)
```

## Loss
We now tackle the loss computation. As expected, PyTorch has us covered once again. There are many loss functions to choose from, depending on the task at hand. Since ours is a regression, we are using the mean squared error (MSE) as loss, and thus we need PyTorch’s nn.MSELoss(). 
Notice that nn.MSELoss() is NOT the loss function itself: We do not pass predictions and labels to it! Instead, as you can see, it returns another function, which we called loss_fn: That is the actual loss function. So, we can pass a prediction and a label to it and get the corresponding loss value:
```python
loss_fn = nn.MSELoss(reduction='mean')
loss_fn
predictions = torch.tensor([0.5, 1.0])
labels = torch.tensor([2.0, 1.3])
loss_fn(predictions, labels)
```

Moreover, you can also specify a reduction method to be applied; that is, how do you want to aggregate the errors for individual points? You can average them (reduction=“mean”) or simply sum them up (reduction=“sum”). In our example, we use the typical mean reduction to compute MSE. If we had used sum as reduction,
we would actually be computing SSE (sum of squared errors).
```python
# Sets learning rate - this is "eta" ~ the "n"-like
# Greek letter
lr = 0.1

# Step 0 - Initializes parameters "b" and "w" randomly
torch.manual_seed(42)
b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)
w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)

# Defines a SGD optimizer to update the parameters
optimizer = optim.SGD([b, w], lr=lr)

# Defines a MSE loss function
loss_fn = nn.MSELoss(reduction='mean')

# Defines number of epochs
n_epochs = 1000

for epoch in range(n_epochs):
    # Step 1 - Computes model's predicted output - forward pass
    yhat = b + w * x_train_tensor
    
    # Step 2 - Computes the loss
    # No more manual loss!
    # error = (yhat - y_train_tensor)
    # loss = (error ** 2).mean()
    loss = loss_fn(yhat, y_train_tensor)

    # Step 3 - Computes gradients for both "b" and "w" parameters
    loss.backward()
    
    # Step 4 - Updates parameters using gradients and
    # the learning rate
    optimizer.step()
    optimizer.zero_grad()
    
print(b, w)
```
```python
loss.detach().cpu().numpy()
```
This seems like a lot of work; there must be an easier way! And there is one, indeed: We can use item(), for tensors with a single element, or tolist() otherwise (it still returns a scalar if there is only one element, though).
```python
print(loss.item(), loss.tolist())
```