```python
import numpy as np
import datetime

import torch
import torch.optim as optim
import torch.nn as nn
import torch.functional as F
from torch.utils.data import DataLoader, TensorDataset, random_split
from torch.utils.tensorboard import SummaryWriter

import matplotlib.pyplot as plt
%matplotlib inline
plt.style.use('fivethirtyeight')
```

## Training Step
The higher-order function that builds a training step function for us is taking, as already mentioned, the key elements of our training loop: model, loss, and optimizer. The actual training step function to be returned will have two arguments, namely, features and labels, and will return the corresponding loss value.
```python
def make_train_step_fn(model, loss_fn, optimizer):
    # Builds function that performs a step in the train loop
    def perform_train_step_fn(x, y):
        # Sets model to TRAIN mode
        model.train()
        
        # Step 1 - Computes our model's predicted output - forward pass
        yhat = model(x)
        # Step 2 - Computes the loss
        loss = loss_fn(yhat, y)
        # Step 3 - Computes gradients for both "a" and "b" parameters
        loss.backward()
        # Step 4 - Updates parameters using gradients and the learning rate
        optimizer.step()
        optimizer.zero_grad()
        
        # Returns the loss
        return loss.item()
    
    # Returns the function that will be called inside the train loop
    return perform_train_step_fn
```
```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Sets learning rate - this is "eta" ~ the "n" like Greek letter
lr = 0.1

torch.manual_seed(42)
# Now we can create a model and send it at once to the device
model = nn.Sequential(nn.Linear(1, 1)).to(device)

# Defines a SGD optimizer to update the parameters (now retrieved directly from the model)
optimizer = optim.SGD(model.parameters(), lr=lr)

# Defines a MSE loss function
loss_fn = nn.MSELoss(reduction='mean')

# Creates the train_step function for our model, loss function and optimizer
train_step_fn = make_train_step_fn(model, loss_fn, optimizer)

# Defines number of epochs
n_epochs = 1000

losses = []

# For each epoch...
for epoch in range(n_epochs):
    # Performs one train step and returns the corresponding loss
    loss = train_step_fn(x_train_tensor, y_train_tensor)
    losses.append(loss)
```

## Dataset

In PyTorch, a dataset is represented by a regular Python class that inherits from the [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class. You can think of it as a list of tuples, each tuple corresponding to one point (features, label).

The most fundamental methods it needs to implement are:
- __init__(self): This takes whatever arguments are needed to build a list of tuples—it may be the name of a CSV file that will be loaded and processed; it may be two tensors, one for features, another one for labels; or anything else, depending on the task at hand.

> There is no need to load the whole dataset in the constructor method (__init__()). If your dataset is large (tens of thousands
> of image files, for instance), loading it all at once would not be memory efficient. It is recommended to load them on demand 
> (whenever __getitem__() is called).
- __getitem__(self, index): This allows the dataset to be indexed so that it can work like a list (dataset[i])—it must return a tuple (features, label) corresponding to the requested data point. We can either return the corresponding slices of our pre-loaded dataset or, as mentioned above, load  them on demand (like in this [example](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class)).
- __len__(self): This should simply return the size of the whole dataset so, whenever it is sampled, its indexing is limited to the actual size.

```python
class CustomDataset(Dataset):
    def __init__(self, x_tensor, y_tensor):
        self.x = x_tensor
        self.y = y_tensor
        
    def __getitem__(self, index):
        return (self.x[index], self.y[index])

    def __len__(self):
        return len(self.x)

# Wait, is this a CPU tensor now? Why? Where is .to(device)?
x_train_tensor = torch.from_numpy(x_train).float()
y_train_tensor = torch.from_numpy(y_train).float()

train_data = CustomDataset(x_train_tensor, y_train_tensor)
print(train_data[0])
```

## TensorDataset
Once again, you may be thinking, “Why go through all this trouble to wrap a couple of tensors in a class?" And, once again, you do have a point… If a dataset is nothing more than a couple of tensors, we can use PyTorch’s TensorDataset class, which will do pretty much the same thing as our custom dataset above.

```python
train_data = TensorDataset(x_train_tensor, y_train_tensor)
print(train_data[0])
```

## DataLoader
Until now, we have used the whole training data at every training step. It has been batch gradient descent all along. This is fine for our ridiculously small dataset, sure, but if we want to get serious about all this, we must use mini-batch gradient descent. Thus, we need mini-batches. Thus, we need to slice our dataset accordingly. Do you want to do it manually?! Me neither!

So we use PyTorch’s DataLoader class for this job. We tell it which dataset to use (the one we just built in the previous section), the desired mini-batch size, and if we’d like to shuffle it or not. That’s it!

> IMPORTANT: in the absolute majority of cases, you should set shuffle=True for your training set to improve the performance
> of gradient descent. There are a few exceptions, though, like time series problems, where shuffling actually leads to data leakage.
> So, always ask yourself: "Do I have a reason NOT to shuffle the data?"
> "What about the validation and test sets?" There is no need to shuffle them since we are not computing gradients with them.
> There is more to a DataLoader than meets the eye—it is also possible to use it together with a sampler to fetch mini-batches  
> that compensate for imbalanced classes, for instance. Too much to handle right now, but we will eventually get there.

Our loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time.

It is typical to use powers of two for mini-batch sizes, like 16, 32, 64, or 128, and 32 seems to be the choice of most people, [Yann LeCun](https://twitter.com/ylecun/status/989610208497360896) included.

```python
train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)
```
To retrieve a mini-batch, one can simply run the command below—it will return a list containing two tensors, one for the features, another one for the labels:
```python
next(iter(train_loader))
```

If you call list(train_loader), you’ll get, as a result, a list of five elements; that is, all five mini-batches. Then you could take the first element of that list to obtain a single mini-batch as in the example above. It would defeat the purpose of using the iterable provided by the DataLoader; that is, to iterate over the elements (minibatches, in that case) one at a time.

## Putting all together

```python
# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors
x_train_tensor = torch.from_numpy(x_train).float()
y_train_tensor = torch.from_numpy(y_train).float()

# Builds Dataset
train_data = TensorDataset(x_train_tensor, y_train_tensor)

# Builds DataLoader
train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)

# Defines number of epochs
n_epochs = 1000

losses = []

# For each epoch...
for epoch in range(n_epochs):
    # inner loop
    mini_batch_losses = []
    for x_batch, y_batch in train_loader:
        # the dataset "lives" in the CPU, so do our mini-batches
        # therefore, we need to send those mini-batches to the
        # device where the model "lives"
        x_batch = x_batch.to(device)
        y_batch = y_batch.to(device)

        # Performs one train step and returns the corresponding loss 
        # for this mini-batch
        mini_batch_loss = train_step_fn(x_batch, y_batch)
        mini_batch_losses.append(mini_batch_loss)

    # Computes average loss over all mini-batches - that's the epoch loss
    loss = np.mean(mini_batch_losses)
    
    losses.append(loss)

print(model.state_dict())
```

## Mini-Batch Inner Loop
From now on, it is very unlikely that you’ll ever use (full) batch gradient descent again, So, it makes sense to, once again, organize a piece of code that’s going to be used repeatedly into its own function: the minibatch inner loop!

The inner loop depends on three elements:
- the device where data is being sent
- a data loader to draw mini-batches from
- a step function, returning the corresponding loss
Taking these elements as inputs and using them to perform the inner loop, we’ll end up with a function like this:

```python
def mini_batch(device, data_loader, step_fn):
    mini_batch_losses = []
    for x_batch, y_batch in data_loader:
        x_batch = x_batch.to(device)
        y_batch = y_batch.to(device)

        mini_batch_loss = step_fn(x_batch, y_batch)
        mini_batch_losses.append(mini_batch_loss)

    loss = np.mean(mini_batch_losses)
    return loss
```

```python
# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors
x_train_tensor = torch.from_numpy(x_train).float()
y_train_tensor = torch.from_numpy(y_train).float()

# Builds Dataset
train_data = TensorDataset(x_train_tensor, y_train_tensor)

# Builds DataLoader
train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)

# Defines number of epochs
n_epochs = 1000

losses = []

# For each epoch...
for epoch in range(n_epochs):
    # inner loop
    loss = mini_batch(device, train_loader, train_step_fn)
    losses.append(loss)

print(model.state_dict())
```

## Random Split
PyTorch’s [random_split()](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) method is an easy and familiar way of performing a training-validation split.

So far, we’ve been using x_train_tensor and y_train_tensor, built out of the original split in Numpy, to build the training dataset. Now, we’re going to be using the full data from Numpy (x and y) to build a PyTorch Dataset first and only then split the data using random_split().

Then, for each subset of data, we’ll build a corresponding DataLoader, so our code will look like this:
```python
torch.manual_seed(13)

# Builds tensors from numpy arrays BEFORE split
x_tensor = torch.from_numpy(x).float()
y_tensor = torch.from_numpy(y).float()

# Builds dataset containing ALL data points
dataset = TensorDataset(x_tensor, y_tensor)

# Performs the split
ratio = .8
n_total = len(dataset)
n_train = int(n_total * ratio)
n_val = n_total - n_train

train_data, val_data = random_split(dataset, [n_train, n_val])

# Builds a loader of each set
train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)
val_loader = DataLoader(dataset=val_data, batch_size=16)
```

## Evaluation
How can we evaluate the model? We can compute the validation loss; that is, how wrong the model’s predictions are for unseen data.
First, we need to use the model to compute predictions and then use the loss function to compute the loss, given our predictions and the true labels.

most important, we need to use the model’s eval() method. The only thing it does is set the model to evaluation mode (just like its train() counterpart did), so the model can adjust its behavior accordingly when it has to perform some operations, like Dropout.

Just like make_train_step_fn(), our new function, make_val_step_fn(), is a higher-order function. Its code looks like this:

```python
def make_val_step_fn(model, loss_fn):
    # Builds function that performs a step in the validation loop
    def perform_val_step_fn(x, y):
        # Sets model to EVAL mode
        model.eval()
        
        # Step 1 - Computes our model's predicted output - forward pass
        yhat = model(x)
        # Step 2 - Computes the loss
        loss = loss_fn(yhat, y)
        # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation
        return loss.item()
    
    return perform_val_step_fn
```
```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Sets learning rate - this is "eta" ~ the "n" like Greek letter
lr = 0.1

torch.manual_seed(42)
# Now we can create a model and send it at once to the device
model = nn.Sequential(nn.Linear(1, 1)).to(device)

# Defines a SGD optimizer to update the parameters (now retrieved directly from the model)
optimizer = optim.SGD(model.parameters(), lr=lr)

# Defines a MSE loss function
loss_fn = nn.MSELoss(reduction='mean')

# Creates the train_step function for our model, loss function and optimizer
train_step_fn = make_train_step_fn(model, loss_fn, optimizer)

# Creates the val_step function for our model and loss function
val_step_fn = make_val_step_fn(model, loss_fn)

# Defines number of epochs
n_epochs = 200

losses = []
val_losses = []

for epoch in range(n_epochs):
    # inner loop
    loss = mini_batch(device, train_loader, train_step_fn)
    losses.append(loss)
    
    # VALIDATION
    # no gradients in validation!
    with torch.no_grad():
        val_loss = mini_batch(device, val_loader, val_step_fn)
        val_losses.append(val_loss)
```
torch.no_grad(): Even though it won’t make a difference in our simple model, it is a good practice to wrap the validation inner
loop with this context manage to disable any gradient computation that you may inadvertently trigger—gradients belong in training, not in validation steps.

## Plotting Losses

```python
fig = plot_losses(losses, val_losses)
```

## TensorBoard 
TensorBoard is a very useful tool, and PyTorch provides classes and methods so that we can integrate it with our model.
```python
%load_ext tensorboard
%tensorboard --logdir runs
```
The magic above tells TensorBoard to look for logs inside the folder specified by the logdir argument: runs. So, there must be a runs folder in the same location as the notebook you’re using to train the model. To make things easier for you, I created a runs folder in the repository, so you get it out-of-the-box.

## SummaryWriter
It all starts with the creation of a [SummaryWriter](https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter):
```python
writer = SummaryWriter('runs/test')
```
> If we do not specify any folder, TensorBoard will default to runs/CURRENT_DATETIME_HOSTNAME, which is not such a great name if you’ll 
> be looking for your experiment results in the future.
> So, it is recommended to name it in a more meaningful way, like runs/test or runs/simple_linear_regression. It will then create a 
> sub-folder inside runs (the folder we specified when we started TensorBoard).
> Even better, you should name it in a meaningful way and add datetime or a sequential number as a suffix, like runs/test_001 or runs/
> test_20200502172130, to avoid writing data of multiple runs into the same folder.

The summary writer implements several methods to allow us to send information to TensorBoard:
- add_graph() 
- add_scalars() 
- add_scalar()
- add_histogram() 
- add_images() 
- add_image()
- add_figure() 
- add_video() 
- add_audio()
- add_text() 
- add_embedding() 
- add_pr_curve()
- add_custom_scalars() 
- add_mesh() 
- add_hparams()

It also implements two other methods for effectively writing data to disk:
- flush()
- close()

We’ll be using the first two methods (add_graph() and add_scalars()) to send our model’s graph (not quite the same as the dynamic computation graph we drew using make_dot(), though), and, of course, both scalars: training and validation losses.
```python
# Fetching a tuple of feature (dummy_x) and label (dummy_y)
dummy_x, dummy_y = next(iter(train_loader))
# Since our model was sent to device, we need to do the same
# with the data.
# Even here, both model and data need to be on the same device!
writer.add_graph(model, dummy_x.to(device))
```

We can use the add_scalars() method to send multiple scalar values at once; it needs three arguments:
- main_tag: the parent name of the tags, or the "group tag," if you will
- tag_scalar_dict: the dictionary containing the key: value pairs for the scalars you want to keep track of (in our case, training and validation losses)
- global_step: step value; that is, the index you’re associating with the values you’re sending in the dictionary; the epoch comes to mind in our case, as losses are computed for each epoch

```python
writer.add_scalars(
    main_tag='loss', 
    tag_scalar_dict={'training': loss, 'validation': val_loss}, 
    global_step=epoch)
```

```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Sets learning rate - this is "eta" ~ the "n" like Greek letter
lr = 0.1

torch.manual_seed(42)
# Now we can create a model and send it at once to the device
model = nn.Sequential(nn.Linear(1, 1)).to(device)

# Defines a SGD optimizer to update the parameters (now retrieved directly from the model)
optimizer = optim.SGD(model.parameters(), lr=lr)

# Defines a MSE loss function
loss_fn = nn.MSELoss(reduction='mean')

# Creates the train_step function for our model, loss function and optimizer
train_step_fn = make_train_step_fn(model, loss_fn, optimizer)

# Creates the val_step function for our model and loss function
val_step_fn = make_val_step_fn(model, loss_fn)

# Creates a Summary Writer to interface with TensorBoard
writer = SummaryWriter('runs/simple_linear_regression')

# Fetches a single mini-batch so we can use add_graph
x_sample, y_sample = next(iter(train_loader))
writer.add_graph(model, x_sample.to(device))

n_epochs = 200

losses = []
val_losses = []

for epoch in range(n_epochs):
    # inner loop
    loss = mini_batch(device, train_loader, train_step_fn)
    losses.append(loss)
    
    # VALIDATION
    # no gradients in validation!
    with torch.no_grad():
        val_loss = mini_batch(device, val_loader, val_step_fn)
        val_losses.append(val_loss)
    
    # Records both losses for each epoch under the main tag "loss"
    writer.add_scalars(main_tag='loss',
                       tag_scalar_dict={'training': loss, 'validation': val_loss},
                       global_step=epoch)

# Closes the writer
writer.close()
```

## Saving and Loading Models
Training a model successfully is great, no doubt about that, but not all models will train quickly, and training may get interrupted (computer crashing, timeout after 12 hours of continuous GPU usage on Google Colab, etc.). It would be a pity to have to start over, right?
So, it is important to be able to checkpoint or save our model, that is, save it to disk, in case we’d like to restart training later or deploy it as an application to make predictions.

### Model State
To checkpoint a model, we basically have to save its state to a file so that it can be loaded back later—nothing special, actually.
What defines the state of a model?
- model.state_dict(): kinda obvious, right?
- optimizer.state_dict(): remember, optimizers have a state_dict() as well
- losses: after all, you should keep track of its evolution
- epoch: it is just a number, so why not? :-)
- anything else you’d like to have restored later

### Saving
Now, we wrap everything into a Python dictionary and use torch.save() to dump it all into a file. Easy peasy! We have just saved our model to a file named model_checkpoint.pth.
```python
checkpoint = {'epoch': n_epochs,
              'model_state_dict': model.state_dict(),
              'optimizer_state_dict': optimizer.state_dict(),
              'loss': losses,
              'val_loss': val_losses}

torch.save(checkpoint, 'model_checkpoint.pth')
```

### Resuming Training
Now we are ready to load the model back, which is easy:
- load the dictionary back using torch.load()
- load model and optimizer state dictionaries back using the load_state_dict() method
- load everything else into their corresponding variables
```python
checkpoint = torch.load('model_checkpoint.pth')

model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

saved_epoch = checkpoint['epoch']
saved_losses = checkpoint['loss']
saved_val_losses = checkpoint['val_loss']

model.train() # always use TRAIN for resuming training
```

### Deploying / Making Predictions
The loading procedure is simpler, though:
- load the dictionary back using torch.load()
- load model state dictionary back using its method load_state_dict()
Since the model is fully trained, we don’t need to load the optimizer or anything else.

```python
checkpoint = torch.load('model_checkpoint.pth')

model.load_state_dict(checkpoint['model_state_dict'])

print(model.state_dict())
```
After recovering our model’s state, we can finally use it to make predictions for new inputs:
```python
new_inputs = torch.tensor([[.20], [.34], [.57]])

model.eval() # always use EVAL for fully trained models!
model(new_inputs.to(device))
```
> After loading a fully trained model for deployment / to make predictions, make sure you ALWAYS set it to evaluation mode:
> model.eval()

### Weird plots in TensorBoard?

Run this if you want to clean up a previous run and start fresh with TensorBoard :-)

```python
import shutil

shutil.rmtree('./runs/simple_linear_regression/', ignore_errors=True)
```

